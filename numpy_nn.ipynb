{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "351da04d-77b6-4e99-9ef4-308cc8e91851",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6334e1a-28bb-48ee-92f2-e299b353d1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = pd.read_csv('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f9bf1df-f36f-4666-89ba-ad600c12688f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57968a14-d053-45be-9082-63eedd3794e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split columns into number columns and columns for classification\n",
    "data_numbers = data.select_dtypes(include=np.number).columns.to_list()\n",
    "data_names = data.select_dtypes(exclude=np.number).columns.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d034ffa1-16ba-4f52-8dd1-f167c0a5b15a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Encoders:\n",
    "    def __init__(self, data, data_names):\n",
    "        self.data = data\n",
    "        self.data_names = data_names\n",
    "        \n",
    "    def ordinal_encoder(self):\n",
    "        for i in self.data_names:\n",
    "            all_values_list = self.data[f'{i}'].values.tolist()\n",
    "            values_list = list(dict.fromkeys(all_values_list))\n",
    "        for k in range(0, len(values_list)):\n",
    "            self.data.loc[data[f'{i}'] == values_list[k], f'{i}'] = k\n",
    "        return self.data.astype('int')\n",
    "\n",
    "encoder = Encoders(data, data_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edb2381-9c0b-4aa0-a13d-d1b01a94800d",
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder.ordinal_encoder()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3376bcbd-b832-49b3-9a45-64f37e2abc5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activators:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def relu(self, z):\n",
    "        return np.maximum(0, z)\n",
    "    \n",
    "    def drelu(self, z):\n",
    "        select = (z >= 1)\n",
    "        z[select] = 1\n",
    "        sel = (z < 0)\n",
    "        z[sel] = 0\n",
    "        return z\n",
    "    \n",
    "    def sigmoid(self, z):\n",
    "        return 1 / (1 + np.exp(-z))\n",
    "    \n",
    "    def dsigmoid(self, z):\n",
    "        return ((2 * np.exp(-z) + 1) / (1 + np.exp(-z))**2)\n",
    "    \n",
    "activator = Activators()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fee12fb-00c8-4565-b6f6-b3dd97290a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalized_data(data, data_numbers, data_names): # sunormuoja skaitines vertes\n",
    "    normalized_all_lists = []\n",
    "    for i in data_numbers:\n",
    "        numbers_list = list(data[f'{i}'])\n",
    "        normalized_list = [(k / (max(data[f'{i}']) - min(data[f'{i}']))) - (min(data[f'{i}']) / (max(data[f'{i}']) - min(data[f'{i}']))) for k in numbers_list]\n",
    "        normalized_all_lists.append(normalized_list)\n",
    "    for j in data_names:\n",
    "        names_list = list(data[f'{j}'])\n",
    "        normalized_all_lists.append(names_list)\n",
    "    transposed_array = np.array(normalized_all_lists).T # grazina transposed array kad stulepliai su eilutem susikeistu vietom\n",
    "    return pd.DataFrame(transposed_array, columns=data.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f769996a-9cdb-438a-b6a8-c82d37b8f991",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_data = normalized_data(data, data_numbers, data_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05e64bfa-9c51-48a6-baf2-8aac60263602",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c78be6e0-910b-4fa9-b740-478803164c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "def layers_units():\n",
    "    layers_number = input('Neural Network layers number (including input and output): ')\n",
    "    layer_units_dic = {'l1': normalized_data_without_output.shape[1]}\n",
    "    for i in range(2, int(layers_number)):\n",
    "        l = input(f'{i} layer units: ')\n",
    "        layer_units_dic.update({f'l{i}': int(l)})\n",
    "    layer_units_dic.update({f'l{int(layers_number)}': 1})\n",
    "    batch_size = input('Batch size: ')\n",
    "    learning_rate = input('Learning rate: ')\n",
    "    out_col = input('Output column name: ')\n",
    "    activator_hidden = input('Choose hidden layers activator, type relu or softmax: ')\n",
    "    activator_output = input('Choose output layer activator, type relu or softmax: ')\n",
    "    return layers_number, layer_units_dic, batch_size, learning_rate, out_col, activator_hidden, activator_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5314329b-bbb9-47ea-9fc7-f6d5b7f368c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "layers_number, layer_units_dic, batch_size, learning_rate, out_col, activator_hidden, activator_output = layers_units()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0995ffbc-517c-4058-bf14-3208464e2e9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized_data_without_output = normalized_data.drop(columns=[out_col])\n",
    "output_data = normalized_data[out_col]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6847e766-50dc-49d7-a3cb-87fe72d885e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork:\n",
    "    def __init__(self, input_data, output_data, layers_number, layer_units_dic, batch_size, activator_hidden, activator_output):\n",
    "        self.data = np.array(normalized_data_without_output)\n",
    "        self.output_data = np.array(output_data).reshape(1, output_data.shape[0])\n",
    "        self.layers_number = layers_number\n",
    "        self.layer_units_dic = layer_units_dic\n",
    "        self.batch_size = int(batch_size)\n",
    "        self.activator_hidden = activator_hidden\n",
    "        self.activator_output = activator_output\n",
    "    \n",
    "    def layer_units(self):\n",
    "        dic_values_list = list(self.layer_units_dic.values())\n",
    "        dic_values_list.pop(0)\n",
    "        dic_values_list.pop(-1)\n",
    "        return dic_values_list\n",
    "        \n",
    "    def weights(self):\n",
    "        layer_units = self.layer_units()\n",
    "        dic = {'W1': np.random.randn(self.data.shape[1], layer_units[0])}\n",
    "        for i in range(2, self.layers_number-1):\n",
    "            W_arrays_list = list(dic.values())\n",
    "            dic.update({f'W{i}': np.random.randn(W_arrays_list[i-2].shape[1], layer_units[i-1])})\n",
    "        W_arrays_list = list(dic.values())\n",
    "        dic.update({f'W{self.layers_number-1}': np.random.randn(W_arrays_list[-1].shape[1], 1)})\n",
    "        weights_list = [i for i in dic.values()]\n",
    "        return weights_list\n",
    "            \n",
    "    def bias(self):\n",
    "        weights_list = self.weights()\n",
    "        bias_list = []\n",
    "        for i in range(0, self.layers_number-1):\n",
    "            bias_list.append(np.random.randn(1, weights_list[i].shape[1]))\n",
    "        assert bias_list[-1].shape[1] == weights_list[-1].shape[1]\n",
    "        return bias_list\n",
    "            \n",
    "    def forward_propagation(self, weights_list, bias_list):\n",
    "        assert weights_list[0].shape[1] == bias_list[0].shape[1]\n",
    "        z_list = [self.data.dot(weights_list[0]) + bias_list[0]]\n",
    "        f_list = [activator.relu(z_list[0])]\n",
    "        for i in range(1, self.layers_number-2):\n",
    "            if self.activator_hidden == 'relu':\n",
    "                z = f_list[i-1].dot(weights_list[i]) + bias_list[i]\n",
    "                z_list.append(z)\n",
    "                f = activator.relu(z)\n",
    "                f_list.append(f)\n",
    "            else:\n",
    "                z = f_list[i-1].dot(weights_list[i]) + bias_list[i]\n",
    "                z_list.append(z)\n",
    "                f = activator.sigmoid(z)\n",
    "                f_list.append(f)\n",
    "        if self.activator_output == 'relu':\n",
    "            z_list.append(f_list[-1].dot(weights_list[-1]) + bias_list[-1])\n",
    "            f_list.append(activator.relu(z_list[-1]))\n",
    "        else:\n",
    "            z_list.append(f_list[-1].dot(weights_list[-1]) + bias_list[-1])\n",
    "            f_list.append(activator.sigmoid(z_list[-1]))\n",
    "        z_list = [i.T for i in z_list]\n",
    "        f_list = [i.T for i in f_list]\n",
    "        return z_list, f_list\n",
    "            \n",
    "    def backward_propagation(self, z_list, f_list, weights_list, bias_list, batch_size):\n",
    "        assert len(z_list) == len(f_list) == len(weights_list) == len(bias_list)\n",
    "        df_list = [f_list[-1] - self.output_data]\n",
    "        dz_list = []\n",
    "        dW_list = []\n",
    "        db_list = []\n",
    "        for i in range(0, self.layers_number-2):\n",
    "            if self.activator_hidden == 'relu':\n",
    "                dz = df_list[i] * activator.drelu(z_list[-(i+1)])\n",
    "                dz_list.append(dz)\n",
    "                dW = 1 / self.batch_size * dz_list[i].dot(f_list[-(i+2)].T)\n",
    "                dW_list.append(dW)\n",
    "                db = 1 / self.batch_size * np.sum(dz_list[i])\n",
    "                db_list.append(db)\n",
    "                df = np.dot(weights_list[-(i+1)], dz_list[i])\n",
    "                df_list.append(df)\n",
    "            else:\n",
    "                dz = df_list[i] * activator.dsigmoid(z_list[-(i+1)])\n",
    "                dz_list.append(dz)\n",
    "                dW = 1 / self.batch_size * dz_list[i].dot(f_list[-(i+2)].T)\n",
    "                dW_list.append(dW)\n",
    "                db = 1 / self.batch_size * np.sum(dz_list[i])\n",
    "                db_list.append(db)\n",
    "                df = np.dot(weights_list[-(i+1)], dz_list[i])\n",
    "                df_list.append(df)\n",
    "        if self.activator_output == 'relu':\n",
    "            dz_list.append(df_list[-1] * activator.drelu(z_list[0]))\n",
    "            dW_list.append(1/self.batch_size * dz_list[-1].dot(self.data))\n",
    "            db_list.append(1/self.batch_size * np.sum(dz_list[-1]))\n",
    "        else:\n",
    "            dz_list.append(df_list[-1] * activator.dsigmoid(z_list[0]))\n",
    "            dW_list.append(1/self.batch_size * dz_list[-1].dot(self.data))\n",
    "            db_list.append(1/self.batch_size * np.sum(dz_list[-1]))\n",
    "        assert len(dW_list) == len(db_list)\n",
    "        new_dW_list = []\n",
    "        new_db_list = []\n",
    "        for i in range(1, len(dW_list)+1):\n",
    "            new_dW_list.append(dW_list[-i])\n",
    "            new_db_list.append(db_list[-i])\n",
    "        dW_list = [i.T for i in new_dW_list]\n",
    "        db_list = [i.T for i in new_db_list]\n",
    "        return dW_list, db_list\n",
    "    \n",
    "    def update_parameters(self, weights_list, bias_list, dW_list, db_list, lr):\n",
    "        assert len(dW_list) == len(db_list) == len(weights_list) == len(bias_list)\n",
    "        assert dW_list[1].shape == weights_list[1].shape\n",
    "        for i in range(0, len(dW_list)):\n",
    "            weights_list[i] -= lr * dW_list[i]\n",
    "            bias_list[i] -= lr * db_list[i]   \n",
    "        return weights_list, bias_list\n",
    "    \n",
    "neural_network = NeuralNetwork(input_data=normalized_data_without_output, \n",
    "                               output_data=output_data, \n",
    "                               layers_number=int(layers_number), \n",
    "                               layer_units_dic=layer_units_dic, \n",
    "                               batch_size=batch_size, \n",
    "                               activator_hidden=activator_hidden, \n",
    "                               activator_output=activator_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00b667c7-3865-4875-b428-5a034d1b5e51",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainNeuralNetwork:\n",
    "    def __init__(self, learning_rate, batch_size, output_data):\n",
    "        self.lr  = learning_rate\n",
    "        self.batch_size = batch_size\n",
    "        self.output_data = np.array(output_data).reshape(1, output_data.shape[0])\n",
    "\n",
    "    def model(self):\n",
    "        weights_list = neural_network.weights()\n",
    "        bias_list = neural_network.bias()\n",
    "        acc_list = []\n",
    "        loss_list = []\n",
    "        for i in range(0, 10):\n",
    "            z_list, f_list = neural_network.forward_propagation(weights_list, bias_list)\n",
    "            dW_list, db_list = neural_network.backward_propagation(z_list, f_list, weights_list, bias_list, self.batch_size)\n",
    "            weights_list, bias_list = neural_network.update_parameters(weights_list, bias_list, dW_list, db_list, self.lr)\n",
    "            accuracy = (f_list[-1] == self.output_data).all(axis=0).mean()\n",
    "            acc_list.append(accuracy)\n",
    "            loss = np.mean((f_list[-1] - self.output_data)**2)\n",
    "            loss_list.append(loss)\n",
    "        return acc_list, loss_list\n",
    "\n",
    "train = TrainNeuralNetwork(learning_rate=float(learning_rate), \n",
    "                           batch_size=int(batch_size), \n",
    "                           output_data=output_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d52619-a51a-46fe-8bd8-b7b20a5acfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.model()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
